<div class="frame">

</div>
<div class="frame">
<p><span>Organization</span> 10 hours</p>
<ul>
<li><p>4h (theory) on cloud computing and architectures + introduction to case study</p></li>
<li><p>3h (lab) case study on-premises</p></li>
<li><p>3h (lab) case study Amazon AWS</p></li>
<li><p>3h seminario (case study vero, pricing, etc.) ???</p></li>
</ul>
</div>
<div class="frame">
<p><span>So far...</span> You’ve practiced with on-premises solutions</p>
<p>Open questions?</p>
<p>toHow do we set up independent services?</p>
<p>toHow do we integrate such services?</p>
<p>toHow do we interface the services?</p>
<p>Let’s guess</p>
<p>toHow would you do that?</p>
<p>toHow much time would it take?</p>
<p>No easy answers</p>
<p>Big-data (distributed) architectures require <em>a lot</em> of skills</p>
<p>toinstallation: how do i cable dozens cluster?</p>
<p>tomanagement: how do i replace a broken disk?</p>
<p>toinstallation: how do i set up a new machine?</p>
<p>toupgrade: how do i extend the cluster with new services/machines?</p>
<p>to(energy and cooling, networking, software licenses, insurance...)</p>
<p>Technological perspective</p>
<p>toit depends on your (team) skills (not only software engineering)</p>
<p>tohow do we orchestrate data flows?</p>
<p>tohow do control resource accesses (e.g., storage)?</p>
<p>tohow do we configure a distributed environment?</p>
<p>Business perspective</p>
<p>tono free lunch, each choice as cost/benefit</p>
<p>tohow much time does it take to master a technology?</p>
<p>tohow many people do i need?</p>
<p>Can we afford to spend resources on tasks are not core for our mission?</p>
<p>tomission: a statement used by a company to explain its purpose(s)</p>
<p>How can I build a working application/data platform?</p>
</div>
<h1 id="cloud-computing-and-big-data">Cloud computing and big data</h1>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [.] Why going cloud</span></p>
</div>
<h2 id="why-going-cloud">Why going cloud</h2>
<div class="frame">
<p><img src="imgs/xkcd_cloud.png" alt="image" /></p>
</div>
<div class="frame">
<p><span>Definition</span></p>
<div class="block">
<p><span>Cloud computing (National Institute of Standards and Technology, NIST)</span> A model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.</p>
</div>
<p>toOn-demand self-service (consume services when you want)</p>
<p>toBroad network access (consume services from anywhere)</p>
<p>toResource pooling (infrastructure, virtual platforms, and applications)</p>
<p>toHaving big pools of resources enables economy of scale</p>
<p>toRapid elasticity (enable horizontal scalability)</p>
<p>toMeasured service (pay for the service you consume as you consume)</p>
</div>
<div class="frame">
<p><span>Why going cloud?</span> <strong>Scalability</strong> that is not possible on premises</p>
<p>toscale from one server to thousands of servers</p>
<p>togrow storage from gigabytes to petabytes</p>
<p>tono longer think about rack space, switches, and power supplies</p>
<p><strong>Reliability</strong></p>
<p>tobuilt to handle failures</p>
<p>tofault-tolerant or highly available</p>
<p><strong>Resource pooling</strong></p>
<p>toenable a resource to serve different consumers</p>
<p>todynamically assigned and reassigned according to demands</p>
<p>toeconomy of scale</p>
<p><strong>Elasticity</strong></p>
<p>toautomated ability to scale resources in response to run-time conditions</p>
<p>tocore justification for the adoption of cloud</p>
<p>Worldwide <strong>deployment</strong></p>
<p>todeploy applications as close to customers as possible</p>
<p>toimprove data locality and privacy</p>
<p>Service <strong>integration</strong></p>
<p>toservices solve common problems (e.g., load balancing, queuing)</p>
<p>todo not reinvent the wheel</p>
<p>User perspective</p>
<p>toeliminate repetitive tasks to focus on strategic ones</p>
<p>toadapt infrastructure to requirements, create (test) environments on demand</p>
<p>toabstract the underlying architecture</p>
<p>Service <strong>integration</strong> and <strong>abstraction</strong> are drivers of change</p>
<p>toFrom databases to data plaforms</p>
<p>toFrom on-premises hardware to serverless architectures</p>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [..] Data platform</span></p>
</div>
<div class="frame">
<p><span>Data platform</span></p>
<p>Companies are collecting huge volumes of data to enable advanced analytics</p>
<p>toData are more and more heterogeneous and complex</p>
<p>toDatabases/warehouses are no longer ideal data hubs for integration/analysis</p>
<p>However,</p>
<p>toRaw data are difficult to obtain, interpret, describe, and maintain</p>
<p>toThere is a need for describing/curating the data to make them consumable</p>
<p>Data lakes (DLs) have increasingly taken the role of such hubs</p>
<p>toEliminate up-front costs of ingestion since data are stored in original format</p>
<p>toOnce in DL, data are available for analysis by everyone in the organization</p>
<p>However,</p>
<p>togetting value from data is not only a matter of storage</p>
<p>toneed integrated and multilevel analytical skills and techniques</p>
<p>Couto et al.: <em>"A DL is a <span>central</span> repository system for <span>storage, processing, and analysis</span> of raw data, in which the data is kept in its original format and is <span>processed to be queried only when needed</span>. It can store a varied amount of formats in big data ecosystems, from unstructured, semi-structured, to structured data sources."</em></p>
<p>Drawing a sharp line been storage/computation/analysis is hard</p>
<p>toIs a database just storage?</p>
<p>toWhat about SQL?</p>
<p>toWhat about OLAP?</p>
<p>Blurring of the architectural borderlines</p>
<p>to“DL” is often replaced by “data platform” or “data ecosystem”</p>
<p>toencompass systems supporting data-intensive storage, computation, analysis</p>
<p>(Cloud-based) Data platform (e.g., Google and Amazon)</p>
<p>torationale: relieve users from complexity of administration and provision</p>
<p>tonot only technological skills, but also privacy, access control, etc.</p>
<p>toonly focus on functional aspects</p>
<p>Are we done? No!</p>
<p>tolacking smart support to govern the complexity of data and transformations</p>
<p>todata transformations must be governed to prevent DP turns into a swamp</p>
<p>toamplified in data science, with data scientists prevailing data architects</p>
<p>toleverage descriptive metadata and maintenance to keep control over data</p>
<p>Data provenance<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>tometadata pertaining to the history of a data item</p>
<p>topipeline including the origin of <strong>objects</strong> and <strong>operations</strong> they are subjected to</p>
<p>towe have a standard: <a href="https://www.w3.org/TR/prov-dm/">https://www.w3.org/TR/prov-dm/</a></p>
<p><img src="imgs/prov.PNG" alt="image" /></p>
<p>Data quality</p>
<p>toMonitoring of the quality (e.g., accuracy) of the objects produced</p>
<p>tonotify when a transformation pipeline is not behaving as expected</p>
<p>Debugging</p>
<p>toInferring the cause of pipeline failures is challenging</p>
<p>toStore inputs of each operation along with</p>
<p>totheir versions</p>
<p>toenvironmental settings (e.g., RAM and CPUs)</p>
<p>And so on...</p>
<p>Are we done? no!</p>
<p>tometadata can become bigger than data themselves</p>
<p>towe need meta meta-data (or models)...</p>
<p>to... chasing our own tails</p>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [..] From PaaS to FaaS (serverless)</span></p>
</div>
<div class="frame">
<p><span>From PaaS to FaaS (serverless)</span></p>
<p>Cloud services are hosted in multiple locations worldwide</p>
<p>toLocations are composed of regions and availability zones</p>
<p>toEach region has multiple availability zones.</p>
<p>toEach region is a separate geographical area</p>
<p>toEach region is completely independent</p>
<p>toAvailability zones in a region are connected through low-latency links</p>
<p>toResources are usually replicated across zones but not regions</p>
<p><img src="imgs/cloud_types.png" alt="image" /></p>
<p>toPublic: accessible to anyone willing to pay (e.g., Microsoft, AWS, Google)</p>
<p>toPrivate: accessible by individuals within an institution</p>
<p>toHybrid: a mix of the previous</p>
<p><img src="imgs/cloud_providers.png" alt="image" /></p>
<p><embed src="imgs/cloud_paastosaas.pdf" /></p>
<p>Understanding architectures is paramount to successful software systems</p>
<p>togood architectures help to scale</p>
<p>topoor architectures cause issues that necessitate a costly rewrite</p>
<p><strong>On premises</strong></p>
<p>toprovisioning, managing, and patching servers is time-consuming</p>
<p>torequire dedicated operations people</p>
<p>toa non-trivial environment is hard to set up and operate effectively</p>
<p>toinfrastructure and hardware are often a distraction from strategic tasks</p>
<p><strong>Infrastructure as a service (IaaS)</strong></p>
<p>toa computing infrastructure provisioned and managed over the internet</p>
<p>toavoid expense/complexity of buying/managing <em>physical</em> servers/data-centers</p>
<p>toIaaS overcomes issues on premises</p>
<p>topossibly requires to manage many environments</p>
<p><strong>Platform as a Service (Paas)</strong></p>
<p>toa development and deployment environment in the cloud</p>
<p>tosupport complete application life-cycle: building, testing, deploying, etc.</p>
<p>toavoid expense/complexity of managing licenses and application infrastructure</p>
<p><strong>Function as a Service (FaaS)</strong></p>
<p>toa coding environment, cloud provider provisions platform to run the code</p>
<p>toinfrastructure provisioning and management are invisible to the developer</p>
<p>toavoid to manage infrastructure</p>
<p><strong>Software as a service (SaaS)</strong></p>
<p>toan application environment</p>
<p>toaccess cloud-based apps over the Internet (e.g., email, Microsoft Office 365)</p>
<p><strong>PaaS</strong> and <strong>containers</strong> are potential solutions to inconsistent infrastructures</p>
<p>to<strong>PaaS</strong> provides a platform for users to run their software</p>
<p>todevelopers write software targeting features/capabilities of the platform</p>
<p>to<strong>containerization</strong> isolates an application with its own environment</p>
<p>tolightweight alternative to full virtualization</p>
<p>tocontainers are isolated but need to be deployed to (public/private) server</p>
<p>toexcellent solution when dependencies are in play</p>
<p>to"housekeeping" challenges and complexities</p>
<p><strong>Serverless</strong></p>
<p>toa software architecture that does not rely on direct access to a server</p>
<p>FaaS is based on a serverless approach</p>
<p>toevery function could be considered as a standalone service</p>
<p>toembodies principles from microservices</p>
<p>tosmall, standalone, fully independent services built for a specific purpose</p>
<p>toeach service written in an appropriate framework and language</p>
<p>tocloud provider is responsible for integration</p>
<p>Principles of FaaS/serverless architectures</p>
<p>touse a compute service to execute code on demand (no servers/containers)</p>
<p>towrite single-purpose stateless functions</p>
<p>tofunctions react to events</p>
<p>todesign push-based, event-driven pipelines</p>
<p>tocreate thicker, more powerful front ends</p>
<p>toembrace third-party services</p>
<p>tosecurity is designed for each function</p>
<p>Write single-purpose stateless functions</p>
<p>tokeep the single responsibility principle in mind</p>
<p>toa function that does just one thing is more testable and robust</p>
<p>toa function with a well-defined interface is also more likely to be reused</p>
<p>tocode should be created in a stateless style</p>
<p>tolocal resources or processes will not survive along sessions</p>
<p>tostatelessness allows scalability</p>
<p>tofunctions that terminate sooner are cheaper</p>
<p>to(pricing is based on #requests, execution time, and allocated memory)</p>
<p>Compose functions in a loose orchestration</p>
<p>tobuild complex but understandable back-end systems</p>
<p>toevent-driven and push-based pipelines</p>
<p>FaaS/Serverless is not a silver bullet</p>
<p>tonot appropriate for latency-sensitive applications</p>
<p>tostrict specific service-level agreements</p>
<p>tovendor lock-in can be an issue for enterprise and government clients</p>
<p>toshould not base mission-critical applications on a public cloud</p>
<p>toLock-in issues</p>
<p>toMigration</p>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [.] Data pipelines on cloud</span></p>
</div>
<h2 id="data-pipelines-on-cloud">Data pipelines on cloud</h2>
<div class="frame">
<p><img src="imgs/xkcd_pipeline.png" alt="image" /></p>
</div>
<div class="frame">
<p><span>Date pipelines on cloud</span> Architecting data pipelines on cloud requires to <strong>standardize/integrate</strong> services</p>
<p>toMake them available through simple portals</p>
<p>toTrack usage/cost with billing mechanism</p>
<p>toMeasure availability</p>
<p>toOrchestrate to meet demand</p>
<p>toProvide a security framework</p>
</div>
<div class="frame">
<p><span>Which services do we need?</span></p>
<p><embed src="imgs/knowledgepyramid.pdf" /></p>
</div>
<div class="frame">
<p><span>A tentative organization</span></p>
<p><embed src="imgs/cloudpatchwork.pdf" /></p>
<p>Not a sharp taxonomy</p>
<p>Ingestion vs Analytics</p>
<p>toData streams are used for ingestion</p>
<p>to... and (event) processing</p>
<p>Storage vs Database</p>
<p>toDatabases are storage</p>
<p>to... with processing capability</p>
<p>to... and with serving capability</p>
<p>Categorizing features (the big-data cube <span class="citation" data-cites="meijer2012your"></span>)</p>
<div class="columns">
<div class="column">
<p><span>0.5</span></p>
<ul>
<li><p>Volume: small to high</p></li>
<li><p>Variety: structure to unstructured</p></li>
<li><p>Velocity: pull to push</p></li>
</ul>
</div><div class="column">
<p><span>0.5</span></p>
<p><embed src="imgs/bigdatacube.pdf" /></p>
</div>
</div>
<div class="columns">
<div class="column">
<p><span>0.5</span> Volume</p>
<ul>
<li><p>Small: small relational DBs</p></li>
<li><p>High: TBs o PBs of data</p></li>
</ul>
</div><div class="column">
<p><span>0.5</span></p>
<p><embed src="imgs/bigdatacube1.pdf" /></p>
</div>
</div>
<div class="columns">
<div class="column">
<p><span>0.5</span> Variety</p>
<ul>
<li><p>structured: relational tuples with FK/PK relationships</p></li>
<li><p>unstructured</p>
<ul>
<li><p>key-value</p></li>
<li><p>columnar</p></li>
<li><p>document-based</p></li>
<li><p>graph</p></li>
</ul></li>
</ul>
</div><div class="column">
<p><span>0.5</span></p>
<p><embed src="imgs/bigdatacube2.pdf" /></p>
</div>
</div>
<div class="columns">
<div class="column">
<p><span>0.5</span> Velocity (latency)</p>
<ul>
<li><p>high: clients synchronously pulling data from sources</p></li>
<li><p>low: sources asynchronously pushing data to clients</p></li>
</ul>
</div><div class="column">
<p><span>0.5</span></p>
<p><embed src="imgs/bigdatacube3.pdf" /></p>
</div>
</div>
<div class="columns">
<div class="column">
<p><span>0.5</span> Our focus (in this course)</p>
<ul>
<li><p>(un)structured big-data batch</p></li>
<li><p>(un)structured big-data streams</p></li>
</ul>
<p><strong>Goal</strong>: keep in mind the cube to categorize the following services</p>
</div><div class="column">
<p><span>0.5</span></p>
<p><embed src="imgs/bigdatacube4.pdf" /></p>
</div>
</div>
<p>AWS</p>
<p><img src="imgs/awspipeline.png" alt="image" /></p>
<p>Google cloud</p>
<p><img src="imgs/gcpipeline.png" alt="image" /></p>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [..] Storage</span></p>
</div>
<div class="frame">
<p><span>Storage</span> <strong>Goal</strong>: persisting data</p>
<p><strong>Storage models</strong></p>
<p>toDifferent ways in which data are organized in a storage system</p>
<p>Understand different storage models to choose the right one</p>
<p>toNature and size of the data</p>
<p>toAnalyses to be performed</p>
<p>toAccess/update frequencies</p>
<p><strong>File system</strong></p>
<p>tostores unstructured binary objects in a tree of directories (folders)</p>
<p>to+ extremely intuitive storage abstraction</p>
<p>to- no support for representing structured data</p>
<p>to- hierarchical organization could not match relationships</p>
<p>to- cannot navigate complex data collections</p>
<p>to- need to maintain consistency as multiple processes read/write a file system</p>
<p>Amazon EFS</p>
<p>todeploy FS within a specific region</p>
<p>toamazon EC2 instances within that region can access that file system</p>
<p>towith amazon vpc/direct connect, mount file system to on-premises machine</p>
<p>tocost</p>
<p>topriced by average number of gb used per month</p>
<p>toif provisioned throughput, also charged by provisioned mb of throughput</p>
<p>topricing also varies according to the region</p>
<p>Google Filestore</p>
<p>todeploy instances within a specific zone</p>
<p>tomount fs instance in any computing instance as long as in the same network</p>
<p>tocost</p>
<p>topriced by gbps and by service tier (standard or premium)</p>
<p>tovaries according to the region</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><strong>Feature</strong></th>
<th style="text-align: center;"><strong>Amazon EFS</strong></th>
<th style="text-align: center;"><strong>Filestore</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Tiers/Modes</td>
<td style="text-align: left;">General Purpose and Max I/O. Each supports either Bursting Throughput or Provisioned Throughput mode.</td>
<td style="text-align: left;">Standard and premium</td>
</tr>
<tr class="even">
<td style="text-align: left;">Deployment locality</td>
<td style="text-align: left;">Regional</td>
<td style="text-align: left;">Zonal</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Protocol</td>
<td style="text-align: left;">NFSv4</td>
<td style="text-align: left;">NFSv3</td>
</tr>
<tr class="even">
<td style="text-align: left;">Encryption</td>
<td style="text-align: left;">Can be enabled at rest and in transit</td>
<td style="text-align: left;">By default, encrypted at rest and in transit</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Pricing</td>
<td style="text-align: left;">Priced by average number of GB used per month and by region of deployment. If using Provisioned Throughput mode, also priced by provisioned MB of throughput.</td>
<td style="text-align: left;">Priced per allocated GB per second, by region of deployment, and by service tier.</td>
</tr>
</tbody>
</table>
<p><strong>Object Store</strong></p>
<p>tostores unstructured binary objects (blobs, binary large object)</p>
<p>tosimplifies the file system model, in general supports a two-level hierarchy</p>
<p>to+ eliminates hierarchy</p>
<p>to+ forbids updates to objects once created</p>
<p>to- little support for organizing data and no support for search</p>
<p>to- user must know an object’s identifier to access it</p>
<p>Cloud Storage and Amazon S3</p>
<p>tostore objects in a bucket</p>
<p>toEach object within a bucket is identified by a unique key within that bucket</p>
<p>toeach object has an associated metadata record</p>
<p>toobject size, date of last modification, and media type</p>
<p>tometadata can be modified</p>
<p>toadd custom metadata</p>
<p>touser experience for buckets is designed to be similar to that of FS</p>
<p>toobject keys are usually paths such as “/foo/subdir/baz.txt”</p>
<p>toprovide filesystem-like APIs—for example, e.g., “ls -R”</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Feature</strong></td>
<td style="text-align: left;"><strong>Amazon S3</strong></td>
<td style="text-align: left;"><strong>Cloud Storage</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Unit of deployment</td>
<td style="text-align: left;">Bucket</td>
<td style="text-align: left;">Bucket</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Deployment identifier</td>
<td style="text-align: left;">Globally unique key</td>
<td style="text-align: left;">Globally unique key</td>
</tr>
<tr class="even">
<td style="text-align: left;">File system emulation</td>
<td style="text-align: left;">Limited</td>
<td style="text-align: left;">Limited</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Obj. metadata</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Obj. versioning</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Update notifications</td>
<td style="text-align: left;">Event notifications</td>
<td style="text-align: left;">Pub/Sub notifications for Cloud Storage, Cloud Storage triggers for Cloud Functions, and object change notifications</td>
</tr>
<tr class="even">
<td style="text-align: left;">Service classes</td>
<td style="text-align: left;">Standard, Standard-Infrequent Access, One Zone-Infrequent Access, Amazon Glacier*</td>
<td style="text-align: left;">Standard, Nearline, Coldline, Archive</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Deployment locality</td>
<td style="text-align: left;">Regional</td>
<td style="text-align: left;">Multi-regional and regional</td>
</tr>
<tr class="even">
<td style="text-align: left;">Pricing</td>
<td style="text-align: left;">Priced by amount of data stored per month, network egress, and number of common API requests</td>
<td style="text-align: left;">Priced by amount of data stored per month, network egress, and number of common API requests</td>
</tr>
</tbody>
</table>
<p>Access frequency/retention determine availability and pricing <img src="imgs/gc_accessfrequency.png" alt="image" /></p>
<p>toStandard: optimized for performance and high frequency access</p>
<p>toNearline: for data accessed less than once a month</p>
<p>toColdline: for data accessed less than once a quarter</p>
<p>toArchive: most cost-effective, data accessed less than once a year</p>
<p>Amazon Glacier</p>
<p>toCost</p>
<p>toAmount of data stored per month</p>
<p>toSize of files stored</p>
<p>toRetrieval type</p>
<p>toNumber of retrieval requests</p>
<p>toNetwork egress</p>
<p>toStorage region</p>
<p>toIf you delete/modify data before the minimum storage period</p>
<p>tocharged for the remainder of the period</p>
<p>todelete an object 5 days after storing the object, charged for remaining 85</p>
<p>Cloud Storage Archive</p>
<p>toCost</p>
<p>topriced by amount of data stored per month and by network egress</p>
<p>toPricing also varies by storage region</p>
<p>toIf you delete/modify data before the minimum storage period</p>
<p>tocharged for the remainder of the period</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>First-byte latency</strong></th>
<th style="text-align: left;"><strong>Minutes to hours</strong></th>
<th style="text-align: left;"><strong>Milliseconds (identical to Cloud Storage Standard)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Retrieval types</td>
<td style="text-align: left;">Expedited, Standard, Bulk. Expedited users can also choose between On-Demand or Provisioned retrieval.</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr class="even">
<td style="text-align: left;">Deployment locality</td>
<td style="text-align: left;">Regional</td>
<td style="text-align: left;">Multi-regional or regional</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Minimum storage period</td>
<td style="text-align: left;">90 days</td>
<td style="text-align: left;">365 days</td>
</tr>
<tr class="even">
<td style="text-align: left;">SLA</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">No</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Pricing</td>
<td style="text-align: left;">Priced by amount of data stored per month, size of files stored, retrieval type, number of retrieval requests, network egress, storage region, storage period, and number of common API requests</td>
<td style="text-align: left;">Priced by amount of data stored per month, network egress, storage region, storage period, and number of common API requests</td>
</tr>
</tbody>
</table>
<p><strong>Database</strong></p>
<p>torelational</p>
<p>toNoSQL</p>
<p>tokey-value</p>
<p>tocolumnar</p>
<p>todocument based</p>
<p>tograph</p>
<p><img src="imgs/gc_storage_analyses.png" alt="image" /></p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Model</strong></th>
<th style="text-align: left;"><strong>Amazon</strong></th>
<th style="text-align: left;"><strong>Google</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Files</td>
<td style="text-align: left;">Elastic File System (EFS), Elastic Block Store (EBS)</td>
<td style="text-align: left;">Google file system</td>
</tr>
<tr class="even">
<td style="text-align: left;">Objects</td>
<td style="text-align: left;">Simple Storage Service (S3)</td>
<td style="text-align: left;">Cloud Storage</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Relational</td>
<td style="text-align: left;">Relational Data Service (RDS), Aurora</td>
<td style="text-align: left;">Cloud SQL, Spanner</td>
</tr>
<tr class="even">
<td style="text-align: left;">NoSQL</td>
<td style="text-align: left;">DynamoDB, HBase</td>
<td style="text-align: left;">Cloud Datastore, Bigtable</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Graph</td>
<td style="text-align: left;">Titan</td>
<td style="text-align: left;">Cayley</td>
</tr>
<tr class="even">
<td style="text-align: left;">Warehouse</td>
<td style="text-align: left;">Redshift</td>
<td style="text-align: left;">BigQuery</td>
</tr>
</tbody>
</table>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [..] Ingestion</span></p>
</div>
<div class="frame">
<p><span>Ingestion</span> <strong>Goal</strong>: moving data to the cloud</p>
<p>Ingestion services, which are used to ingest data from a source environment into a reliable and stable target environment or data type.</p>
<p>Moving data to the cloud</p>
<p>to of data to move, <span>1Gbps</span> connection to the internet</p>
<p>to?</p>
<p>to / <span>(1Gbps / 8)</span> / <span>60 / 60 / 24</span> =̃ a week without internet</p>
<p><strong>Batch/Bulk</strong></p>
<p>toMove data from on-premises storage</p>
<p>Workflow</p>
<p>toreceive shipment</p>
<p>toset up</p>
<p>totransfer data</p>
<p>toship back ( shipping carrier)</p>
<p>AWS Snowball</p>
<p>toAWS Snowball comes in 50TB (North America only) and 80TB versions</p>
<p>toAWS Snowball is not rack-mountable</p>
<p>toThroughput</p>
<p>to1 Gbps or 10 Gbps using an RJ-45 connection</p>
<p>to10 Gbps using a fiber optic connection</p>
<p>Google Transfer Appliance</p>
<p>to100TB version known as the TA100, 480TB version known as the TA480</p>
<p>toTA100 comes in a 2U rack-mountable, TA480 is not rack-mountable</p>
<p>toThroughput</p>
<p>to1 Gbps or 10 Gbps using an RJ-45 connection</p>
<p>to10 Gbps using a fiber optic connection</p>
<p>to4 ethernet ports, adaptive load balancing for multi-stream throughput</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Feature</strong></th>
<th style="text-align: left;"><strong>AWS Snowball</strong></th>
<th style="text-align: left;"><strong>Transfer Appliance</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Capacity per unit</td>
<td style="text-align: left;">50 TB or 80 TB</td>
<td style="text-align: left;">100 TB or 480 TB</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="3">Maximum transfer rate</td>
<td style="text-align: left;" rowspan="3">10Gbps</td>
<td style="text-align: left;">20Gbps for TA100</td>
</tr>
<tr class="odd">
<td style="text-align: left;">40Gbps for TA480</td>
</tr>
<tr class="even">
<td style="text-align: left;">Both with automatic link aggregation</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Email status updates?</td>
<td style="text-align: left;">No</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="2">Rack-mountable?</td>
<td style="text-align: left;" rowspan="2">No</td>
<td style="text-align: left;">Yes for TA100</td>
</tr>
<tr class="odd">
<td style="text-align: left;">No for TA480</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="2">Use fee</td>
<td style="text-align: left;">$200 for 50 TB</td>
<td style="text-align: left;">$300 for TA100</td>
</tr>
<tr class="odd">
<td style="text-align: left;">$250 for 80 TB</td>
<td style="text-align: left;">$1800 for TA480</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="2">Daily fee</td>
<td style="text-align: left;" rowspan="2">$15/day after 10 days</td>
<td style="text-align: left;">$30/day after 10 days for TA100</td>
</tr>
<tr class="odd">
<td style="text-align: left;">$90/day after 25 days for TA480</td>
</tr>
<tr class="even">
<td style="text-align: left;">Transfer modes</td>
<td style="text-align: left;">Push</td>
<td style="text-align: left;">Push or pull</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Transfer data out of object store?</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">No</td>
</tr>
</tbody>
</table>
<p><strong>Stream</strong></p>
<p>toReal-time streaming data</p>
<p><strong>Event</strong>: anything that we can observe occurring at a particular point in time</p>
<p><strong>Continuous streaming</strong>: an illimitated succession of individual events, ordered by the point in time at which each event occurred</p>
<p><strong>Publish/subscribe (pub/sub)</strong>: a way of communicating messages</p>
<p>toSenders publish messages associated with one or more topics</p>
<p>toReceivers subscribe to specific topics, receive all messages with that topic</p>
<p>toMessages are events</p>
<div class="block">
<p><span>Unified log</span></p>
<p>Unified, append-only, ordered, distributed log that allows the centralization of continuous event streams</p>
</div>
<p>General idea:</p>
<p>toCollect events from disparate source systems</p>
<p>toStore them in a unified log</p>
<p>toEnable data processing applications to operate on these event streams</p>
<p><strong>Unified</strong>: a single deployment of this technology a company, with multiple applications sending events to it and reading events from it</p>
<p>toLog serves as central data backbone</p>
<p>toUnified log can contain many distinct continuous streams of events</p>
<p>toNot all events are sent to the same event stream</p>
<p><strong>Append-only</strong>: new events are appended to the unified log</p>
<p>toexisting events are never updated in place</p>
<p>toif read the event #10, never look at events 1 through 10 again</p>
<p>toEvents are automatically deleted from the unified log when they age</p>
<p><strong>Distributed</strong>: the unified log lives across a cluster of machines</p>
<p>toStill, the log is unified since we have a single (conceptual)</p>
<p>toScalability: work with streams larger than the capacity of single machines</p>
<p>toDurability: replicate all events within the cluster to overcome data loss</p>
<p>toDivide events in a given stream into *shards* (partitions)</p>
<p><img src="imgs/eventstream_shard.jpg" alt="image" /></p>
<p><strong>Ordered</strong>: events in a shard have a sequential IDs (<em>offset</em>, unique within a shard)</p>
<p>toLocal ordering keeps things much simpler</p>
<p>toApplications maintain their own cursor for each shard</p>
<p><img src="imgs/eventstream_order.jpg" alt="image" /></p>
<p>Two types of processing can be performed on a single event stream</p>
<p>to<strong>Single-event</strong>, a single event produces zero or more events</p>
<p>to<em>Validating</em> "Does this event contain all the required fields?”</p>
<p>to<em>Enriching</em> "Where is this IP address located?"</p>
<p>to<em>Filtering</em> "Is this error critical?"</p>
<p>to<strong>Multiple-event</strong>, multiple events collectively produce zero or more events</p>
<p>to<em>Aggregating</em>, functions such as minimum, maximum, sum</p>
<p>to<em>Pattern matching</em>, looking for patterns or co-occurence</p>
<p>to<em>Sorting</em>, reordering events based on a sort key</p>
<p><img src="imgs/eventstream_processing.jpg" alt="image" /></p>
<p>Amazon Kinesis Data Streams</p>
<p>tostreaming model to ingest data</p>
<p>toproducers send data to a stream that you create and provision by shard</p>
<p>toeach shard provides a maximum of 1 MBps and 1000 data puts per second</p>
<p>toThis data is stored in data records that consist of the following:</p>
<p>toAn incremental sequence number</p>
<p>toA user-supplied partition key</p>
<p>toload-balance records across shards</p>
<p>toA data blob</p>
<p>toBy default, records are retained for 24 hours (maximum of 7 days)</p>
<p>toData order</p>
<p>tomaintains order through partition key and sequence number</p>
<p>toproducer provides a partition key that determines the shard</p>
<p>toThe shard adds an incremental sequence number to the record</p>
<p>toConsumers get records by shard, records ordered by sequence number</p>
<p>toordering is not guaranteed for requests across shards</p>
<p>toOperations</p>
<p>tousers must scale shards up and down manually</p>
<p>tomonitor usage with Amazon CloudWatch and modify scale as needed</p>
<p>toscaling process is called resharding,</p>
<p>tosplit shard into two, or merge two shards</p>
<p>toavoid shard management by using Kinesis Data Firehose</p>
<p>toautomate management to aggregating data into S3/Redshift</p>
<p>toKinesis is a regional service, with streams scoped to specific regions</p>
<p>toall ingested data must travel to the region in which the stream is defined.</p>
<p>toCosts</p>
<p>topriced by shard hour, data volume, and data retention period</p>
<p>topay for resources you provision (even if not used)</p>
<p>toAmazon Kinesis Data Firehose is priced by data volume.</p>
<p>Google Pub/Sub</p>
<p>tomessaging service that uses a publisher/subscriber model</p>
<p>tocreate a Pub/Sub topic, publish data to that topic</p>
<p>toapplications subscribe to the topic to retrieve ingested data</p>
<p>toAvoid definition of of shards.</p>
<p>topush model or pull model</p>
<p>topush model, the Pub/Sub server sends a request to the subscriber application at a preconfigured URL endpoint</p>
<p>topull model, the subscriber application requests messages from the server, and then acknowledges receipt</p>
<p>toeach data (i.e., message) must be base64-encoded and no larger than 10 MB</p>
<p>toAt the time of ingestion, Pub/Sub adds a messageId attribute and a publishTime</p>
<p>tomessageId unique within the topic</p>
<p>toData order</p>
<p>todelivers messages on a best-effort basis, using system-supplied publishTime</p>
<p>todoes not guarantee only-once or in-order delivery: on occasion, a message might be delivered more than once, and out of order</p>
<p>tosubscriber should be idempotent when processing messages and able to handle messages out of order</p>
<p>toachieve stricter ordering by using application-supplied sequence numbers</p>
<p>toPub/Sub does not require provisioning, and handles sharding, replication, and scaling</p>
<p>todon’t need to use partition keys—Pub/Sub manages data partitioning on your behalf</p>
<p>toless overhead, but fewer guarantees about message ordering.</p>
<p>toPub/Sub uses Google’s HTTP(S) load balancer to support data ingestion globally across all Google Cloud regions</p>
<p>toload balancer automatically directs the traffic to Pub/Sub servers in an appropriate region in order to minimize latency.</p>
<p>toPub/Sub is priced by data volume</p>
<p>todoes not require resource provisioning, you pay for only the resources you consume</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Feature</strong></td>
<td style="text-align: left;"><strong>Amazon Kinesis Data Streams</strong></td>
<td style="text-align: left;"><strong>Google Pub/Sub</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Unit of deployment</td>
<td style="text-align: left;">Stream</td>
<td style="text-align: left;">Topic</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Unit of provisioning</td>
<td style="text-align: left;">Shard</td>
<td style="text-align: left;">N/A (fully managed)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data unit</td>
<td style="text-align: left;">Record</td>
<td style="text-align: left;">Message</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data source</td>
<td style="text-align: left;">Producer</td>
<td style="text-align: left;">Publisher</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data destination</td>
<td style="text-align: left;">Consumer</td>
<td style="text-align: left;">Subscriber</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data partitioning</td>
<td style="text-align: left;">User-supplied partition key</td>
<td style="text-align: left;">N/A (fully managed)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Retention period</td>
<td style="text-align: left;">Up to 7 days</td>
<td style="text-align: left;">Up to 7 days</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data delivery order</td>
<td style="text-align: left;">Service-supplied sequence key (best effort)</td>
<td style="text-align: left;">Service-supplied publish time (best effort)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Max data size</td>
<td style="text-align: left;">1 MB</td>
<td style="text-align: left;">10 MB</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Deployment locality</td>
<td style="text-align: left;">Regional</td>
<td style="text-align: left;">Global</td>
</tr>
<tr class="even">
<td style="text-align: left;">Pricing model</td>
<td style="text-align: left;">Per shard-hour, PUT payload units, and optional data retention</td>
<td style="text-align: left;">Message ingestion and delivery, and optional message retention</td>
</tr>
</tbody>
</table>
</div>
<div class="frame">
<p><span>AWS: ingestion</span></p>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [.] Structural patterns for data pipelines</span></p>
</div>
<h2 id="structural-patterns-for-data-pipelines">Structural patterns for data pipelines</h2>
<div class="frame">
<p><span>Structural patterns</span> Patterns are architectural solutions to problems in software design</p>
<p>toAddress common problems in software development</p>
<p>toCommand pattern</p>
<p>toMessaging pattern</p>
<p>toPriority queue pattern</p>
<p>toPipes and filters pattern</p>
</div>
<div class="frame">
<p><span>Command pattern</span> encapsulate a request as an object, thereby letting you parameterize clients with different requests, queue or log requests, and support undoable operations” because of the “need to issue requests to objects without knowing anything about the operation being requested or the receiver of the request”</p>
<p><img src="imgs/pattern_command.PNG" alt="image" /></p>
</div>
<div class="frame">
<p><span>Pipes and filters pattern</span> Decompose a complex processing task into a sequence of manageable services</p>
<p>toComponents designed to transform data are referred to as filters</p>
<p>toConnectors that pass data between components are referred to as pipes</p>
<p><img src="imgs/pattern_pipeline.PNG" alt="image" /></p>
</div>
<div class="frame">
<p><span>Messaging pattern</span> Decoupling functions and services from direct dependence on one another and allowing storage of events/records/requests in a queue. The reliability comes from the fact that if the consuming service goes offline, messages are retained in the queue and can still be processed at a later time.</p>
<p>toDepending on how the system is designed, a message queue can have a single sender/receiver or multiple senders/receivers.</p>
<p><img src="imgs/pattern_messaging.PNG" alt="image" /></p>
</div>
<div class="frame">
<p><span>Priority queue pattern</span> Control how and when messages are dealt with</p>
<p>toDifferent queues, topics, or streams to feed messages to your functions</p>
<p>toHigh-priority messages go through expensive services with more capacity</p>
<p><img src="imgs/pattern_priority.PNG" alt="image" /></p>
</div>
<h1 id="data-pipeline-on-aws">Data pipeline on AWS</h1>
<div class="frame">
<p><span>AWS</span> Useful links and sources</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Resource</strong></th>
<th style="text-align: left;"><strong>Link</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">AWS Educate</td>
<td style="text-align: left;"><a href="https://aws.amazon.com/it/education/awseducate/">https://aws.amazon.com/it/education/awseducate/</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">AWS Console</td>
<td style="text-align: left;"><a href="https://console.aws.amazon.com/console/home?region=us-east-1">https://console.aws.amazon.com/console/home?region=us-east-1</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">IAM</td>
<td style="text-align: left;"><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/iam-ug.pdf">https://docs.aws.amazon.com/IAM/latest/UserGuide/iam-ug.pdf</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">SDK</td>
<td style="text-align: left;"><a href="https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/home.html">https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/home.html</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Lambda</td>
<td style="text-align: left;"><a href="https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html">https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html</a></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions">https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Kinesis</td>
<td style="text-align: left;"><a href="https://docs.aws.amazon.com/streams/latest/dev/introduction.html">https://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">DynamoDB</td>
<td style="text-align: left;"><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html</a></td>
</tr>
</tbody>
</table>
<p>Amazon Web Services (AWS) is a platform of web services</p>
<p>tocomputing, storing, and networking, at different abstraction layers</p>
<p>toweb service means services controlled via a (visual) web interface</p>
<p>toEC2, which offers virtual servers, and S3, which offers storage capacity</p>
<p>toE.g., Linux server with an optimized distribution called Amazon Linux</p>
<p>toVirtual servers can fail, so you need at least two of them</p>
<p>toThe load balancer will distribute the traffic between them</p>
<p>Clouds are often divided into types</p>
<p>toPublic, public usage</p>
<p>toPrivate, private usage</p>
<p>toHybrid, a mixture of a public and a private cloud AWS is a public cloud</p>
<p>Cloud computing services also have several classifications:</p>
<p>toInfrastructure as a service (IaaS)</p>
<p>tofundamental resources like computing, storage, and networking</p>
<p>toPlatform as a service (PaaS)</p>
<p>todeploy applications to cloud platforms (AWS Elastic Beanstalk, Heroku)</p>
<p>toSoftware as a service (SaaS)</p>
<p>toprovide software running in the cloud (Amazon WorkSpaces, and Microsoft Office 365)</p>
<p>AWS product portfolio contains IaaS, PaaS, and SaaS</p>
</div>
<div class="frame">
<p><span>Accessing the cloud</span> Most cloud services can be accessed in multiple ways. First, most support access via the web, thus permitting intuitive point and click access without any programming or even local software installation (beyond a web browser) on your part. The availability of such intuitive interfaces is part of the attraction of cloud services.</p>
<p>A web interface becomes tedious if the same or similar actions must be performed repeatedly. In such cases, you likely want to write programs that issue requests to cloud services on your behalf. Fortunately, most cloud services support such programmatic access. Typically, they support a Representational State Transfer (REST) application programming interface (API) that permits requests to be transmitted via the secure Hypertext Transfer Protocol (HTTPS) that is used by web browsers. (This common use of HTTPS is not a coincidence: the web interfaces discussed in the first paragraph are often implemented via browser-hosted Javascript programs that generate such REST messages.) REST APIs are the key to programmatic interactions with cloud services.</p>
<p>One way to interact with cloud services programmatically is to write programs that generate REST messages directly. However, while constructing REST messages “by hand” may appeal to hardcore system programmers, you will normally want to access cloud services via software development kits (SDKs) that you install on your computer. Such SDKs permit access from programming languages such as Python (our choice in this book), C++, Go, Java, PHP, and Ruby.</p>
</div>
<div class="frame">
<p><span>Interacting with AWS</span></p>
<p>toManagement Console (web-based)</p>
<p>toCommand-line interface</p>
<p>toSDK, call AWS from within your application</p>
<p>toBlueprints, a system description containing all services and dependencies <img src="imgs/aws_blueprint.PNG" alt="image" /></p>
</div>
<div class="frame">
<p><span>Creating an infrastructure</span> Infrastructure as code Infrastructure as code describes the idea of using a high-level programming language to control IT systems. In software development tools like automated tests, code repositories, and build servers are increasing the quality of software engineering. If your infrastructure can be treated as code, you can apply the same techniques to infrastructure code that you do to your application code. In the end, you’ll improve the quality of your infrastructure by using automated tests, code repositories, and build servers. WARNING Don’t mix up the terms infrastructure as code and infrastructure as a service (IaaS)! IaaS means renting servers, storage, and network with a pay-peruse pricing model.</p>
<p>If you want to use cloud advantages like scaling the number of servers depending on the current load or building a highly available infrastructure, you’ll need to start new virtual servers several times a day. On top of that, the number of virtual servers you’ll have to supply with updates will grow. The steps required to deploy an application don’t change, but as figure 5.1 shows, you need to perform them on multiple servers. Deploying software manually to a growing number of servers becomes impossible over time and has a high risk of human failure. This is why we recommend that you automate the deployment of applications.</p>
<p>A simple but powerful and flexible way of automating application deployment is to run a script on server startup. To go from a plain OS to a fully installed and configured server, you need to follow these three steps: 1 Start a plain virtual server containing just an OS. 2 Execute a script at the end of the boot process. 3 Install and configure your applications with the help of a script.</p>
</div>
<div class="frame">
<p><span>Service categorization</span> Main service categories</p>
<p>toCompute services, computing power and memory (e.g., virtual servers)</p>
<p>toApp services, solutions for common use cases (message queues, topics, and searching)</p>
<p>toDeployment and administration, grant/revoke access, monitor servers, deploy applications.</p>
<p>toStorage, collect and persist data</p>
<p>toNetworking, define private networks, DNS, etc.</p>
</div>
<div class="frame">
<p><span>Cloud providers</span> Comparing alternatives AWS isn’t the only cloud computing provider. Microsoft and Google have cloud offerings as well. OpenStack is different because it’s open source and developed by more than 200 companies including IBM, HP, and Rackspace. Each of these companies uses Open- Stack to operate its own cloud offerings, sometimes with closed source add-ons. You could run your own cloud based on OpenStack, but you would lose most of the benefits outlined in section 1.3. Comparing cloud providers isn’t easy, because open standards are mostly missing. Functionality like virtual networks and message queuing are realized differently. If you know what features you need, you can compare the details and make your decision.</p>
</div>
<div class="frame">
<p><span>Data pipeline on AWS cloud</span> <embed src="imgs/pipeline_aws.pdf" /></p>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [.] Authentication and authorization</span></p>
</div>
<h2 id="authentication-and-authorization">Authentication and authorization</h2>
<div class="frame">
<p><span>AWS - Identity and Access Management</span></p>
<p><strong>Identity and Access Management (IAM)</strong></p>
<p>toweb service that controls access to AWS resources</p>
<p>toIAM controls who is <em>authenticated</em> and <em>authorized</em> to use resources</p>
<p><strong>User</strong></p>
<p>tounique identity recognized by AWS services and applications</p>
<p>toindividual, system, or application accessing AWS services</p>
<p>tosimilar to user in an operating system like Windows or UNIX</p>
<p>After the account creation</p>
<p>tobegin with a single sign-in identity that has complete access to all AWS services and resources in the account</p>
<p>toi.e., a <code>root user</code></p>
<p>todo not use the root user for your everyday tasks, even the administrative ones</p>
<p>touse the root user only to create your first IAM user</p>
<p>tospecify permissions to control which operations a user can perform</p>
<p>What can a user do?</p>
<p>toplace requests to web services such as Amazon S3 and Amazon EC2</p>
<p>toIf permitted, a user has access to all of the resources under the AWS account</p>
<p>tosers can make requests to AWS services using security credentials</p>
<p>to<strong>Explicit</strong> permissions govern a user’s ability to call AWS services</p>
<p><strong>IAM role</strong></p>
<p>toset of permissions for making AWS service requests</p>
<p>totrusted entities (e.g., such as IAM users) assume roles</p>
<p>to<strong>not</strong> associated with a specific user or group</p>
<p>todelegate access with defined permissions to trusted entities without having to share long-term access keys</p>
<p>tothere is no limit to the number of IAM roles you can assume</p>
<p>Role vs user</p>
<p>touser has permanent long-term credentials and is used to directly interact with AWS services</p>
<p>torole does not have any credentials and cannot make direct requests to AWS services</p>
<p>toIAM roles are meant to be assumed by authorized entities, such as IAM users, applications, or an AWS service such as EC2.</p>
<p><strong>Policy</strong></p>
<p>toan object in AWS that, when associated with an identity or resource, defines their permissions</p>
<p>toAWS evaluates these policies when an IAM principal (user or role) makes a request</p>
<p>toPermissions in the policies determine whether the request is allowed or denied</p>
<p>toYou manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles)</p>
<p>tosix types of policies (listed in order of frequency)</p>
<p>toIdentity-based policies, grant permissions to an identity.</p>
<p>toResource-based policies, e.g. resource-based policies are Amazon S3 bucket policies</p>
<p>toPermissions boundaries, maximum permissions that the identity-based policies can grant to an entity</p>
<p>toService control policy (SCP), maximum permissions for members of an organization or organizational unit (OU)</p>
<p>toAccess control lists (ACLs), which accounts can access the resource to which the ACL is attached.</p>
<p>toSession policies, limit the permissions that the role or user’s identity-based policies grant to the session</p>
</div>
<div class="frame">
<p><span>AWS CLI and authentication</span> We will mainly refer to the CLI interface</p>
<pre><code>Synopsis
********
   aws [options] &lt;command&gt; &lt;subcommand&gt; [parameters]

Description
***********
A unified tool to manage your AWS services.</code></pre>
<p>First of all it is necessary to install the CLI (version 2)</p>
<p>toSee <span><a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html">https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html</a></span></p>
<p>This is your AWS account</p>
<p><img src="imgs/aws_account.PNG" alt="image" /></p>
<p>Click on “Account Details” to get your secrets</p>
<p>toCopy the content into the file <code>~/.aws/configure</code></p>
<p>to<strong>All examples assume that you have setup your credentials in the credentials file</strong> <img src="imgs/aws_account1.PNG" alt="image" /></p>
<p>Run <code>aws configure</code></p>
<p>toConfirm <code>AWS Access Key ID</code></p>
<p>toConfirm <code>AWS Secret Access Key</code></p>
<p>toSet <code>Default region name</code> to <code>us-east-1</code></p>
<p>toSet <code>Default output format</code> to <code>json</code></p>
<p>It is also possible to configure an AWS profile</p>
<p>toA (named) profile is a collection of settings and credentials</p>
<p>toIf profile is specified, its settings and credentials are used to run a command</p>
<p>toWhen no profile is explicitly referenced, use “default”</p>
<p>to<strong>We stick to “default”</strong></p>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [.] NoSQL</span></p>
</div>
<h2 id="nosql">NoSQL</h2>
<div class="frame">
<p><span>AWS - NoSQL with DynamoDB</span></p>
<p>The following are the basic DynamoDB components:</p>
<p><img src="imgs/HowItWorksPeople.png" alt="image" /></p>
<p><strong>Tables</strong></p>
<p>toa collection of (data) items</p>
<p>toe.g., example table called People</p>
<p><strong>Items</strong></p>
<p>toa group of attributes that is uniquely identifiable among all others</p>
<p>toEach table contains zero or more items</p>
<p>tono limit to the number of items you can store in a table.</p>
<p>toItems tuples in other database systems</p>
<p>toIn the People table, each item represents a person.</p>
<p>toEach item in the table has a unique identifier, or primary key</p>
<p>toIn the People table, the primary key consists of one attribute (PersonID)</p>
<p><strong>Attributes</strong></p>
<p>toa fundamental data element that is not broken down any further</p>
<p>toe.g., an item in the People table contains attributes PersonID, LastName, FirstName</p>
<p>toMost of the attributes are scalar (have only one value)</p>
<p>toStrings and numbers are common examples of scalars</p>
<p>toSome of the items have a nested attribute (Address)</p>
<p>toup to 32 levels deep</p>
<p><strong>Schemaless</strong></p>
<p>toOther than the primary key, the People table is schemaless</p>
<p>toneither the attributes nor their data types need to be defined beforehand</p>
<p>toEach item can have its own distinct attributes.</p>
<p><strong>Primary Key</strong></p>
<p>toTo create a table, you must specify the primary key of the table</p>
<p>toThe primary key uniquely identifies each item in the table,</p>
<p>tono two items can have the same key.</p>
<p>Two types of primary keys</p>
<p>toPartition key: a simple primary key, composed of one attribute known as the partition key</p>
<p>tokey values as inputs to an internal hash function</p>
<p>toThe hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored</p>
<p>toaccess any item in the People table directly by providing the PersonId</p>
<p>tocomposite primary key: Partition key and sort key (two attributes)</p>
<p>tofirst attribute is the partition key</p>
<p>tosecond attribute is the sort key</p>
<p>toitems in same partition key value are stored together, in sorted order by sort key</p>
<p><strong>Secondary Indexes</strong></p>
<p>toone or more secondary indexes per table</p>
<p>toquery data using an alternate key (additionally to queries against primary key)</p>
<p>toindexes are automatically maintained on add, update, or delete</p>
<p>Two types of indexes</p>
<p>to<strong>Global secondary</strong> has partition and sort keys different from those on table</p>
<p>to<strong>Local secondary</strong> has the same partition key but a different sort key</p>
<p>toEach table has a quota of 20 global and 5 local indexes</p>
<p>How do we shape the schema?</p>
<p>to<a href="https://cloud.google.com/bigtable/docs/schema-design">https://cloud.google.com/bigtable/docs/schema-design</a></p>
<pre><code>$ aws dynamodb create-table \
    --table-name soil-humidity \
    --attribute-definitions AttributeName=field,AttributeType=S AttributeName=timestamp,AttributeType=S \
    --key-schema AttributeName=field,KeyType=HASH AttributeName=timestamp,KeyType=RANGE \
    --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1
    
$ aws dynamodb create-table \
    --table-name interpolate-soil-humidity \
    --attribute-definitions AttributeName=field,AttributeType=S AttributeName=timestamp,AttributeType=S \
    --key-schema AttributeName=field,KeyType=HASH AttributeName=timestamp,KeyType=RANGE \
    --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1</code></pre>
<p>Amazon DynamoDB is available in multiple AWS Regions around the world. Each Region is independent and isolated from other AWS Regions. For example, if you have a table called People in the us-east-2 Region and another table named People in the us-west-2 Region, these are considered two entirely separate tables. For a list of all the AWS Regions in which DynamoDB is available, see AWS Regions and Endpoints in the Amazon Web Services General Reference.</p>
<p>Every AWS Region consists of multiple distinct locations called Availability Zones. Each Availability Zone is isolated from failures in other Availability Zones, and provides inexpensive, low-latency network connectivity to other Availability Zones in the same Region. This allows rapid replication of your data among multiple Availability Zones in a Region.</p>
<p>When your application writes data to a DynamoDB table and receives an HTTP 200 response (OK), the write has occurred and is durable. The data is eventually consistent across all storage locations, usually within one second or less.</p>
<p>DynamoDB supports eventually consistent and strongly consistent reads.</p>
<p>Eventually Consistent Reads</p>
<p>When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data.</p>
<p>Strongly Consistent Reads</p>
<p>When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful. However, this consistency comes with some disadvantages:</p>
<p>A strongly consistent read might not be available if there is a network delay or outage. In this case, DynamoDB may return a server error (HTTP 500).</p>
<p>Strongly consistent reads may have higher latency than eventually consistent reads.</p>
<p>Strongly consistent reads are not supported on global secondary indexes.</p>
<p>Strongly consistent reads use more throughput capacity than eventually consistent reads. For details, see Read/Write Capacity Mode</p>
<p>Note</p>
<p>DynamoDB uses eventually consistent reads, unless you specify otherwise. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation.</p>
<p>If you choose provisioned mode, you specify the number of reads and writes per second that you require for your application. You can use auto scaling to adjust your table’s provisioned capacity automatically in response to traffic changes. This helps you govern your DynamoDB use to stay at or below a defined request rate in order to obtain cost predictability.</p>
<p>Provisioned mode is a good option if any of the following are true:</p>
<p>You have predictable application traffic.</p>
<p>You run applications whose traffic is consistent or ramps gradually.</p>
<p>You can forecast capacity requirements to control costs.</p>
<p>Read Capacity Units and Write Capacity Units</p>
<p>For provisioned mode tables, you specify throughput capacity in terms of read capacity units (RCUs) and write capacity units (WCUs):</p>
<p>One read capacity unit represents one strongly consistent read per second, or two eventually consistent reads per second, for an item up to 4 KB in size. Transactional read requests require two read capacity units to perform one read per second for items up to 4 KB. If you need to read an item that is larger than 4 KB, DynamoDB must consume additional read capacity units. The total number of read capacity units required depends on the item size, and whether you want an eventually consistent or strongly consistent read. For example, if your item size is 8 KB, you require 2 read capacity units to sustain one strongly consistent read per second, 1 read capacity unit if you choose eventually consistent reads, or 4 read capacity units for a transactional read request. For more information, see Capacity Unit Consumption for Reads.</p>
<p>Note</p>
<p>To learn more about DynamoDB read consistency models, see Read Consistency.</p>
<p>One write capacity unit represents one write per second for an item up to 1 KB in size. If you need to write an item that is larger than 1 KB, DynamoDB must consume additional write capacity units. Transactional write requests require 2 write capacity units to perform one write per second for items up to 1 KB. The total number of write capacity units required depends on the item size. For example, if your item size is 2 KB, you require 2 write capacity units to sustain one write request per second or 4 write capacity units for a transactional write request. For more information, see Capacity Unit Consumption for Writes.</p>
<p>Putting data</p>
<pre><code>$ aws dynamodb create-table \
    --table-name soil-humidity \
    --attribute-definitions AttributeName=field,AttributeType=S AttributeName=timestamp,AttributeType=S \
    --key-schema AttributeName=field,KeyType=HASH AttributeName=timestamp,KeyType=RANGE \
    --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1

$ aws dynamodb create-table \
    --table-name interpolate-soil-humidity \
    --attribute-definitions AttributeName=field,AttributeType=S AttributeName=timestamp,AttributeType=S \
    --key-schema AttributeName=field,KeyType=HASH AttributeName=timestamp,KeyType=RANGE \
    --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1

$ aws dynamodb delete-table --table-name soil-humidity

$ aws dynamodb delete-table --table-name interpolate-soil-humidity

$ aws dynamodb list-tables

$ aws dynamodb put-item \
    --table-name soil-humidity \
    --item \
    &#39;{&quot;field&quot;: {&quot;S&quot;: &quot;field-01&quot;}, &quot;timestamp&quot;: {&quot;S&quot;: &quot;1611226870&quot;}, &quot;xx&quot;: {&quot;N&quot;: &quot;0&quot;}, &quot;yy&quot;: {&quot;N&quot;: &quot;-20&quot;}, &quot;value&quot;: {&quot;N&quot;: &quot;-17&quot;}}&#39;

$ aws dynamodb put-item \
    --table-name soil-humidity \
    --item \
    &#39;{&quot;field&quot;: {&quot;S&quot;: &quot;field-01&quot;}, &quot;timestamp&quot;: {&quot;N&quot;: &quot;1611226880&quot;}, &quot;xx&quot;: {&quot;S&quot;: &quot;0&quot;}, &quot;yy&quot;: {&quot;S&quot;: &quot;-20&quot;}, &quot;value&quot;: {&quot;S&quot;: &quot;-20&quot;}}&#39;
    
$ aws kinesis put-record --stream-name events --partition-key soilhumidity --cli-binary-format raw-in-base64-out --data \
    &#39;{&quot;field&quot;: &quot;field-01&quot;, &quot;timestamp&quot;: &quot;1611226890&quot;, &quot;xx&quot;: &quot;0&quot;, &quot;yy&quot;: &quot;-20&quot;, &quot;value&quot;: &quot;-23&quot;, &quot;message&quot;: &quot;Hello from AWS CLI!&quot;}&#39;

$ aws kinesis put-record --stream-name events --partition-key soilhumidity --cli-binary-format raw-in-base64-out --data \
    &#39;{&quot;field&quot;: &quot;field-01&quot;, &quot;timestamp&quot;: &quot;1611226900&quot;, &quot;xx&quot;: &quot;0&quot;, &quot;yy&quot;: &quot;-20&quot;, &quot;value&quot;: &quot;-26&quot;, &quot;message&quot;: &quot;Hello from AWS CLI!&quot;}&#39;</code></pre>
<p>The Query operation in Amazon DynamoDB finds items based on primary key values.</p>
<p>You must provide the name of the partition key attribute and a single value for that attribute. Query returns all items with that partition key value. Optionally, you can provide a sort key attribute and use a comparison operator to refine the search results.</p>
<p>To specify the search criteria, you use a key condition expression—a string that determines the items to be read from the table or index.</p>
<p>You must specify the partition key name and value as an equality condition.</p>
<p>You can optionally provide a second condition for the sort key (if present). The sort key condition must use one of the following comparison operators:</p>
<p>a = b — true if the attribute a is equal to the value b</p>
<p>a &lt; b — true if a is less than b</p>
<p>a &lt;= b — true if a is less than or equal to b</p>
<p>a &gt; b — true if a is greater than b</p>
<p>a &gt;= b — true if a is greater than or equal to b</p>
<p>a BETWEEN b AND c — true if a is greater than or equal to b, and less than or equal to c.</p>
<p>The following function is also supported:</p>
<p>begins_with (a, substr)— true if the value of attribute a begins with a particular substring.</p>
<pre><code>$ aws dynamodb query \
    --table-name soil-humidity \
    --key-condition-expression &quot;field = :n&quot; \
    --expression-attribute-values &#39;{&quot;:n&quot;:{&quot;S&quot;:&quot;field-01&quot;}}&#39;

$ aws dynamodb query \
    --table-name soil-humidity \
    --key-condition-expression &quot;field = :n&quot; \
    --expression-attribute-values &#39;{&quot;:n&quot;:{&quot;S&quot;:&quot;field-02&quot;}}&#39;
    
aws dynamodb delete-table --table-name soil-humidity

aws dynamodb delete-table --table-name interpolate-soil-humidity

aws dynamodb create-table \
  --table-name soil-humidity \
  --attribute-definitions AttributeName=field,AttributeType=S AttributeName=timestamp,AttributeType=S \
  --key-schema AttributeName=field,KeyType=HASH AttributeName=timestamp,KeyType=RANGE \
  --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1

aws dynamodb create-table \
  --table-name interpolate-soil-humidity \
  --attribute-definitions AttributeName=field,AttributeType=S AttributeName=sensorid,AttributeType=S \
  --key-schema AttributeName=field,KeyType=HASH AttributeName=sensorid,KeyType=RANGE \
  --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1

aws dynamodb query \
  --table-name soil-humidity \
  --key-condition-expression &quot;field = :n&quot; \
  --expression-attribute-values &#39;{&quot;:n&quot;:{&quot;S&quot;:&quot;field-01&quot;}}&#39;

aws dynamodb query \
  --table-name soil-humidity \
  --key-condition-expression &quot;field = :n&quot; \
  --expression-attribute-values &#39;{&quot;:n&quot;:{&quot;S&quot;:&quot;field-02&quot;}}&#39;

aws dynamodb query \
  --table-name soil-humidity \
  --key-condition-expression &quot;field = :n&quot; \
  --expression-attribute-values &#39;{&quot;:n&quot;:{&quot;S&quot;:&quot;field-00&quot;}}&#39;

aws dynamodb query \
  --table-name interpolate-soil-humidity \
  --key-condition-expression &quot;field = :n&quot; \
  --expression-attribute-values &#39;{&quot;:n&quot;:{&quot;S&quot;:&quot;field-00&quot;}}&#39;</code></pre>
<p>Filter Expressions for Query</p>
<p>If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded.</p>
<p>A filter expression is applied after a Query finishes, but before the results are returned. Therefore, a Query consumes the same amount of read capacity, regardless of whether a filter expression is present.</p>
<p>A Query operation can retrieve a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.</p>
<p>A filter expression cannot contain partition key or sort key attributes. You need to specify those attributes in the key condition expression, not the filter expression.</p>
<p>The syntax for a filter expression is identical to that of a condition expression. Filter expressions can use the same comparators, functions, and logical operators as a condition expression, with the addition of the not-equals operator (&lt;&gt;). For more information, see Condition Expressions.</p>
<p>Example</p>
<p>The following AWS CLI example queries the Thread table for a particular ForumName (partition key) and Subject (sort key). Of the items that are found, only the most popular discussion threads are returned—in other words, only those threads with more than a certain number of Views.</p>
<pre><code>
$ aws dynamodb query \
    --table-name Thread \
    --key-condition-expression &quot;ForumName = :fn and Subject = :sub&quot; \
    --filter-expression &quot;#v &gt;= :num&quot; \
    --expression-attribute-names &#39;{&quot;#v&quot;: &quot;Views&quot;}&#39; \
    --expression-attribute-values file://values.json
 </code></pre>
<p>The arguments for –expression-attribute-values are stored in the values.json file.</p>
<pre><code>{
    &quot;:fn&quot;:{&quot;S&quot;:&quot;Amazon DynamoDB&quot;},
    &quot;:sub&quot;:{&quot;S&quot;:&quot;DynamoDB Thread 1&quot;},
    &quot;:num&quot;:{&quot;N&quot;:&quot;3&quot;}
}</code></pre>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [.] Event streams</span></p>
</div>
<h2 id="event-streams">Event streams</h2>
<div class="frame">
<p><span>AWS - Event streams with Kinesis</span> Creating a Kinesis stream</p>
<p>Kinesis Data Stream</p>
<p>toA Kinesis data stream is a set of shards.</p>
<p>toEach shard has a sequence of data records.</p>
<p>toEach data record has a sequence number that is assigned by Kinesis Data Streams.</p>
<p>Data Record</p>
<p>tounit of data stored in a Kinesis data stream.</p>
<p>toData records are composed of a sequence number, a partition key, and a data blob</p>
<p>toKinesis Data Streams does not inspect, interpret, or change the data in the blob in any way.</p>
<p>toA data blob can be up to 1 MB.</p>
<p>Retention Period</p>
<p>tothe length of time that data records are accessible after they are added to the stream</p>
<p>toA stream’s retention period is set to a default of 24 hours after creation.</p>
<p>toAdditional charges apply for streams with a retention period set to more than 24 hours. For more information, see Amazon Kinesis Data Streams Pricing</p>
<p>Producer</p>
<p>toProducers put records into Amazon Kinesis Data Streams</p>
<p>Consumer</p>
<p>toGet and process records from Amazon Kinesis Data Streams</p>
<p>toAlso known as Amazon Kinesis Data Streams Application</p>
<p>Two types of consumers that you can develop</p>
<p>toshared fan-out consumers</p>
<p>toenhanced fan-out consumers</p>
<p>toThe output of a Kinesis Data Streams application can be input for another stream, enabling you to create complex topologies that process data in real time.</p>
<p>toAn application can also send data to a variety of other AWS services.</p>
<p>toThere can be multiple applications for one stream, and each application can consume data from the stream independently and concurrently.</p>
<p>Shard</p>
<p>tosequence of data records in a stream</p>
<p>toA stream is composed of one or more shards, each of which provides a fixed unit of capacity.</p>
<p>toEach shard can support up to 5 transactions per second for reads, up to a maximum total data read rate of 2 MB per second and up to 1,000 records per second for writes, up to a maximum total data write rate of 1 MB per second (including partition keys). The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards.</p>
<p>If your data rate increases, you can increase or decrease the number of shards allocated to your stream. For more information, see Resharding a Stream.</p>
<p>Partition Key</p>
<p>toA partition key is used to group data by shard within a stream. Kinesis Data Streams segregates the data records belonging to a stream into multiple shards. It uses the partition key that is associated with each data record to determine which shard a given data record belongs to. Partition keys are Unicode strings, with a maximum length limit of 256 characters for each key. An MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards using the hash key ranges of the shards. When an application puts data into a stream, it must specify a partition key.</p>
<pre><code>$ aws kinesis create-stream --stream-name events --shard-count 2

$ aws kinesis delete-stream --stream-name events

$ aws kinesis describe-stream --stream-name events

$ aws kinesis get-shard-iterator --stream-name=events \
    --shard-id=shardId-000000000000 --shard-iterator-type=TRIM_HORIZON
{
    &quot;ShardIterator&quot;: &quot;iterator-id-00&quot;
}

$ aws kinesis get-records --shard-iterator &quot;iterator-id-00&quot;
{
    &quot;Records&quot;: [],
    &quot;NextShardIterator&quot;: &quot;iterator-id-01&quot;,
    &quot;MillisBehindLatest&quot;: 0
}

$ aws kinesis get-records --shard-iterator  &quot;iterator-id-01&quot;
{
    &quot;Records&quot;: [],
    &quot;NextShardIterator&quot;: &quot;iterator-id-02&quot;,
    &quot;MillisBehindLatest&quot;: 0
}</code></pre>
<p>Created a stream with two shards</p>
<p>toevents sent will be written to one or either of the two shards</p>
<p>tostream processing apps read events from all shards</p>
<p>toStatus <span><a href="https://console.aws.amazon.com/kinesis/home?region=us-east-1#/streams/list">https://console.aws.amazon.com/kinesis/home?region=us-east-1#/streams/list</a></span></p>
<p>We specified that the shard iterator should be of type TRIM_HORIZON. This is AWS jargon for the oldest events in the shard that have not yet been trimmed—expired for being too old. At the time of writing, records are trimmed from a Kinesis stream after a fixed period of 24 hours</p>
<p>Creating the producer using Amazon SDK</p>
<p>toneed to be able to send events to it from all our various client applications</p>
<p>toE.g., Producer in Python using Amazon SDK (boto3)</p>
</div>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [.] Serverless</span></p>
</div>
<h2 id="serverless">Serverless</h2>
<div class="frame">
<p><span>Going serverless</span> Amazon AWS Lambda as a case study</p>
<p>toExecute code in a massively parallelized way in response to events</p>
<p>toElastic Compute Cloud (EC2) servers run the code</p>
<p>toE.g., Linux server with distribution Amazon Linux</p>
<p>toSee also Microsoft Azure Functions, IBM Bluemix, Google Cloud Functions</p>
<p>AWS lambda</p>
<p>toThe Lambda runtime invokes a lambda function multiple times in parallel</p>
<p>toInvocation supports push/pull event models</p>
<p>to<strong>Lambda function</strong>: code + configuration + dependencies</p>
<p>toCompute service that executes code written in JavaScript/Python/C#/Java</p>
<p>toSource code (JARs or DLLs) is zipped up and deployed to a container</p>
<p>In AWS, every Lambda function is a granular service</p>
<p>toInputs and outputs should be clearly defined (i.e., a clear interface)</p>
<p>toSimilar to the compute-as-glue architecture we described previously</p>
<p>toMake sure your function follows the single-responsibility principle</p>
<p>toMake the function idempotent (i.e., given an input produce the same output)</p>
<p>toClearly define an interface for the function</p>
<p>toMake sure inputs and outputs are clearly stated</p>
<p>toFunction are black-boxes</p>
<p>toconsumer should not have to know how it works</p>
</div>
<div class="frame">
<p><span>AWS - Serverless with Lambda</span> AWS Lambda is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second. You pay only for the compute time that you consume—there is no charge when your code is not running. With Lambda, you can run code for virtually any type of application or backend service, all with zero administration. Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, code monitoring and logging.</p>
<p>When using Lambda, you are responsible only for your code. Lambda manages the compute fleet that offers a balance of memory, CPU, network, and other resources. This is in exchange for flexibility, which means you cannot log in to compute instances, or customize the operating system on provided runtimes. These constraints enable Lambda to perform operational and administrative activities on your behalf, including provisioning capacity, monitoring fleet health, applying security patches, deploying your code, and monitoring and logging your Lambda functions.</p>
<p>Create a function</p>
<p>to<a href="https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions">https://console.aws.amazon.com/lambda/home?region=us-east-1#/functions</a></p>
<p>The following "create-function" example creates a Lambda function named "my-function".</p>
<pre><code>$ aws iam create-role --role-name lambda-execute

$ aws iam attach-role-policy \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaKinesisExecutionRole --role-name lambda-execute

$ aws iam attach-role-policy \
    --policy-arn arn:aws:iam::aws:policy/AWSLambdaInvocation-DynamoDB --role-name lambda-execute

$ aws iam attach-role-policy \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaDynamoDBExecutionRole --role-name lambda-execute

$ aws iam attach-role-policy \
    --policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess --role-name lambda-execute

$ aws iam attach-role-policy \
    --policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccesswithDataPipeline --role-name lambda-execute

$ aws lambda create-function \
    --function-name my-function \
    --runtime nodejs10.x \
    --zip-file fileb://my-function.zip \
    --handler my-function.handler \
    --role rolename</code></pre>
<p>to<code>zip-file</code> deployment package that contains code and dependencies</p>
<p>to<code>handler</code> name of the method that Lambda calls to execute your function</p>
<pre><code>import boto3
import time
def lambda_handler(event, context):
    client = boto3.resource(&#39;dynamodb&#39;) # create dynamodb resource
    table = client.Table(&quot;soil-humidity&quot;) # search for dynamoDB table
    table.put_item(Item={
            &quot;field&quot;: &quot;field-01&quot;, # partition key
            &quot;timestamp&quot;: str(int(time.time())), # sort key
            &quot;xx&quot;: &quot;0&quot;, &quot;yy&quot;: &quot;-20&quot;, &quot;value&quot;: &quot;-25&quot;,
            &quot;message&quot;: &quot;Hello from Lambda!&quot;})</code></pre>
</div>
<h1 id="migration-case-study">Migration: Case Study</h1>
<div class="frame">
<p><br />
</p>
<hr />
<p><br />
<span> [.] Migrating to AWS</span></p>
</div>
<h2 id="migrating-to-aws">Migrating to AWS</h2>
<div class="frame">
<p><span>Migration</span></p>
<p>Goals</p>
<p>toEvaluating the costs for a cloud/on-premises data platform</p>
<p>toFill in this table</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Cost</th>
<th style="text-align: center;">On-premises</th>
<th style="text-align: center;">On cloud</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Hardware</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">?</td>
</tr>
<tr class="even">
<td style="text-align: center;">Software</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">?</td>
</tr>
</tbody>
</table>
<p>toReal-world case study</p>
<figure>
<embed src="imgs/migration.pdf" /><figcaption aria-hidden="true">On-premises (reference) architecture</figcaption>
</figure>
<figure>
<embed src="imgs/migration0.pdf" /><figcaption aria-hidden="true">Hardware requirements</figcaption>
</figure>
<pre><code>lshw -short -C cpu
lshw -short -C memory
lshw -short -C disk</code></pre>
<p>Software stack</p>
<p>toClassic Hadoop stack plus Python and Docker</p>
<p>Software cost <span>(up to 2020): 0€</span></p>
<p>toFree Cloudera Management System</p>
<p>toNo software licensing (for research purpose)</p>
<p>Hardware cost <span>(up to Mar 05, 2021): ?</span></p>
<p>to<a href="https://www.rect.coreto-europe.com/">https://www.rect.coreto-europe.com/</a></p>
<p><img src="imgs/migration_hw_onprem.PNG" alt="image" /></p>
<p>Software cost (up to 2020): 0€</p>
<p>toFree Cloudera Management System</p>
<p>toNo software licensing (for research purpose)</p>
<p>Hardware cost (up to Mar 05, 2021): <span class="math inline">1767€ ⋅ 18 = 31806€</span></p>
<p>to<a href="https://www.rect.coreto-europe.com/">https://www.rect.coreto-europe.com/</a></p>
<p>toAmortization over 3 years (i.e., 10602€/year)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Cost</th>
<th style="text-align: center;">On-premises</th>
<th style="text-align: center;">On cloud</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Hardware</td>
<td style="text-align: center;">10602€/year</td>
<td style="text-align: center;">?</td>
</tr>
<tr class="even">
<td style="text-align: center;">Software</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">?</td>
</tr>
</tbody>
</table>
<p>Software cost <span>(up to Mar 05, 2021): 10000€/year <span class="math inline"> ⋅ 18</span> = 180000€/year</span></p>
<p>to</p>
<p>to</p>
<p>to</p>
<p>toNo license for research purpose</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Cost</th>
<th style="text-align: center;">On-premises</th>
<th style="text-align: center;">On cloud</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Hardware</td>
<td style="text-align: center;">10602€/year</td>
<td style="text-align: center;">?</td>
</tr>
<tr class="even">
<td style="text-align: center;">Software</td>
<td style="text-align: center;"><span>180000 €/year</span></td>
<td style="text-align: center;">?</td>
</tr>
</tbody>
</table>
<p><em>“Houston we’ve had a problem!”</em></p>
<p>toWe cannot update/extend the cluster anymore</p>
<p>Moving a Hadoop cluster to the cloud (we only consider AWS)</p>
<p>toAWS price calculator <a href="https://calculator.aws/#/estimate">https://calculator.aws/#/estimate</a></p>
<p>How do we start?</p>
<p>towe need architectural/application requirements</p>
<p>towe already defined the hardware and the software stack</p>
<p>tostart with coarse tuning, identify the dominating costs first</p>
<p>tois it computing, storage, or processing?</p>
<p>toidentify a suitable budget, implement, refine later</p>
<p>towrong refinements can do a lot of damage</p>
<p>Migrate the cluster as-is: 13500$/month = 162000$/year</p>
<p>to18 EC2 instances (t4g.2xlarge), 12TB EBS storage each machine</p>
<p>toStill, we have no software stack configuration</p>
<p><img src="imgs/migration_aws_asonprem1.PNG" alt="image" /></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Cost</th>
<th style="text-align: center;">On-premises</th>
<th style="text-align: center;">On cloud</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Hardware</td>
<td style="text-align: center;">10602€/year</td>
<td style="text-align: center;"><span>162000$/year</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Software</td>
<td style="text-align: center;">180000€/year</td>
<td style="text-align: center;">?</td>
</tr>
</tbody>
</table>
<p>toAlso, pick the right region</p>
<p><img src="imgs/migration_aws_asonprem.PNG" alt="image" /></p>
<p>It makes no sense to move the cluster as-is</p>
<p>tomore machines ensure better scalability but higher costs</p>
<p>tolet’s think about some optimization</p>
<p>towe need have some minimum software requirements</p>
<p>tohow many machines do we need at minimum?</p>
<figure>
<embed src="imgs/migration1.pdf" /><figcaption aria-hidden="true">Storage</figcaption>
</figure>
<p><strong>HDFS</strong></p>
<p>toHow much durability do we need? (i.e., how many replicas)</p>
<p>toWe can think about a durability decreasing over time</p>
<p>toHP1: two replicas for fresh data</p>
<p>toHP2: move cold data to a glacier or delete id</p>
<p>toIs it cheaper to store data or (re)process them?</p>
<p><strong>HBase</strong> has marginal effects on the pricing (<span class="math inline">100<em>G</em><em>B</em> ≪ 50<em>T</em><em>B</em></span>)</p>
<p>toFor simplicity, we can omit it (focus on the high costs)</p>
<p><strong>Overall</strong>: 50TB storage/year</p>
<figure>
<embed src="imgs/migration2.pdf" /><figcaption aria-hidden="true">Processing</figcaption>
</figure>
<p>Processing takes place each time that ESA provides a satellite image</p>
<p>toSome days no images are available</p>
<p>toSome days up to 10 images are available</p>
<p>toEach image is about 1GB</p>
<p>toProcessing produces new images with about the same same</p>
<p>toSpark jobs are always executed with the same parameters</p>
<p><strong>Image processing</strong></p>
<p>to4 machines, 2 cores and 10GB RAM at least</p>
<p><strong>Weather processing</strong> is negligible, we can omit it</p>
<p>How do we proceed with the migration?</p>
<p>Try to achieve the smallest migration impact</p>
<p>tofind the most similar cloud-based solution to a Hadoop cluster</p>
<p>torethink applications when you got the know-how</p>
<p>Rethinking cloud-based applications (business processes) means</p>
<p>tounderstand the application requirements of each process</p>
<p>torethink all the applications in a cluster fashion</p>
<p>tomuch harder</p>
<p>Compare 4 machines on-premises vs on cloud</p>
<p>On-premises</p>
<p>to4 machines: 196$/month = 2356$/year</p>
<p>toCloudera requires at least 10 nodes: 100000$/year</p>
<p>Migrate the cluster with minimal requirements: 1367$/month = 16404$/year</p>
<p>to4 EC2 instances (t4g.2xlarge), 12TB EBS storage each machine</p>
<p>toStill, we have no software stack configuration</p>
<p><img src="imgs/migration_aws_asonprem2.PNG" alt="image" /></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Cost</th>
<th style="text-align: center;">On-premises</th>
<th style="text-align: center;">On cloud</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Hardware</td>
<td style="text-align: center;">2356€/year</td>
<td style="text-align: center;"><span>16404$/year</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Software</td>
<td style="text-align: center;">100000€/year</td>
<td style="text-align: center;">?</td>
</tr>
</tbody>
</table>
<p>Amazon EMR is the most similar service to Cloudera</p>
<p>toCreate/delete a cluster at need</p>
<p>toComputational nodes (based on EC2)</p>
<p>to<em>master</em> node, manages the cluster (always active)</p>
<p>to<em>core</em> nodes, computation + data</p>
<p>toinclude the HDFS demon (this cannot be turned off)</p>
<p>toHard to scale</p>
<p>to<em>task</em> nodes: core nodes without data demon</p>
<p>towrite to S3, not to HDFS</p>
<p>toEasy to (auto)scale</p>
<p>toDecoupling processing and storage, we loose data locality</p>
<p>Migrating cluster to EMR: 14710€/year</p>
<p>to</p>
<p>to1 x Master EMR nodes, EC2 (m4.xlarge), Utilization (75 h/month): 4.5€</p>
<p>to1 x Core EMR nodes, EC2 (m4.xlarge), Utilization (75 h/month): 4.5€</p>
<p>to4 x Task EMR nodes, EC2 (m4.4xlarge), Utilization (75 h/month): 72€</p>
<p>to4 x EC2 on demand (task node): 174.83€</p>
<p>toStorage amount (30 GB)</p>
<p>toWorkload (Daily, Duration of peak: 0 Hr 15 Min)</p>
<p>toInstance type (m4.xlarge)</p>
<p>to2 x EC2 on demand (master and core nodes): 330€</p>
<p>toStorage amount (30 GB)</p>
<p>toInstance type (m4.xlarge)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Cost</th>
<th style="text-align: center;">On-premises</th>
<th style="text-align: center;">On cloud</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Hardware</td>
<td style="text-align: center;">2356€/year</td>
<td style="text-align: center;" rowspan="2">14710€/year</td>
</tr>
<tr class="even">
<td style="text-align: center;">Software</td>
<td style="text-align: center;">100000€/year</td>
</tr>
</tbody>
</table>
<p>Can we do better?</p>
<p>On-Demand Instance</p>
<p>topay for compute capacity by the hour (minimum of 60 seconds)</p>
<p>tono long-term commitments</p>
<p>Spot Instance</p>
<p>tounused EC2 instance that is available for less than the on-demand price</p>
<p>tohourly price is called a spot price</p>
<p>toadjusted based on long-term supply and demand for spot instances</p>
<p>toruns when capacity is available and price lower than threshold</p>
<p>towhen data-cetenter resources are low, spot instances are dropped</p>
<p>toonly suitable for batch workloads</p>
<p><em>Capacity-optimized</em> strategy</p>
<p>toallocated instances into the most available pools</p>
<p>tolook at real-time capacity data, predict which are the most available</p>
<p>toworks well for workloads such as big data and analytics</p>
<p>toworks well when we have high cost of interruption</p>
<p><em>Lowest-price</em> strategy</p>
<p>toallocates instances in pools with lowest price at time of fulfillment</p>
<p>Migrating cluster to EMR: 13445€/year</p>
<p>toS3 Infrequent Access storage (50 TB per month): 640€</p>
<p>to1 Master EMR nodes, EC2 (m4.xlarge), Utilization (75 h/month): 4.5€</p>
<p>to1 Core EMR nodes, EC2 (m4.xlarge), Utilization (75 h/month): 4.5€</p>
<p>to4 Task EMR nodes, EC2 (m4.4xlarge), Utilization (75 h/month): 72€</p>
<p>to4 x EC2 spot (task node): 69.55€</p>
<p>toStorage amount (30 GB)</p>
<p>toWorkload (Daily, Duration of peak: 0 Hr 15 Min)</p>
<p>toInstance type (m4.xlarge)</p>
<p>to2 x EC2 on demand (master and core nodes): 330€</p>
<p>toStorage amount (30 GB)</p>
<p>toInstance type (m4.xlarge)</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Cost</th>
<th style="text-align: center;">On-premises</th>
<th style="text-align: center;">On cloud</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Hardware</td>
<td style="text-align: center;">2356€/year</td>
<td style="text-align: center;" rowspan="2">13445€/year</td>
</tr>
<tr class="even">
<td style="text-align: center;">Software</td>
<td style="text-align: center;">100000€/year</td>
</tr>
</tbody>
</table>
<p>Can we do better?</p>
<p>toPick ad-hoc cloud services</p>
<p>toE.g., AWS Lambda e AWS Batch (container Docker)</p>
<p>to... to re-build the applications (food for thoughts)</p>
</div>
<section id="references" class="frame">
<h3>References</h3>
</section>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><em>Data provenance</em> and <em>data lineage</em> are used in the literature as synonyms or with slightly-different flavors<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
